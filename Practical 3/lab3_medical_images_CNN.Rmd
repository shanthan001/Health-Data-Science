```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As we found for natural language processing, R is a little bit limited when it comes to deep neural networks. Unfortunately, these networks are the current gold-standard methods for medical image analysis. The ML/DL R packages that do exist provide interfaces (APIs) to libraries built in other languages like C/C++ and Java. This means there are a few “non-reproducible” installation and data gathering steps to run first. It also means there are a few “compromises” to get a practical that will A) be runnable B) actually complete within the allotted time and C) even vaguely works.
Doing this on a full real dataset would likely follow a similar process (admittedly with more intensive training and more expressive architectures). Google colab running an R kernel hasn't proven a very effective alternative thus far either.

# Install packages

In this practical, we will be performing image classification on real medical images using the Torch deep learning library and several packages to manage and preprocess the images. We will utilize the following packages for our task:

`gridExtra`: This package provides functions for arranging multiple plots in a grid layout, allowing us to visualize and compare our results effectively.

`jpeg`: This package enables us to read and write JPEG images, which is a commonly used image format.

`imager`: This package offers a range of image processing functionalities, such as loading, saving, resizing, and transforming images.

`magick`: This package provides an interface to the ImageMagick library, allowing us to manipulate and process images using a wide variety of operations.

To ensure that the required packages are available, we can install them using the `install.packages()` function. Additionally, some packages may require additional dependencies to be installed. In such cases, we can use the `BiocManager::install()` function from the Bioconductor project to install the necessary dependencies.

Here is an example code snippet to install the required packages:
you can uncomment the following code to install required packages
```{r}
# Install necessary packages
#install.packages("gridExtra")
#install.packages("jpeg")
#install.packages("imager")
#install.packages("magick")

# Install Bioconductor package (if not already installed)
#if (!require("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")

# Install EBImage package from Bioconductor
#BiocManager::install("EBImage")
#install.packages("abind")

# Install torch, torchvision, and luz
#install.packages("torch")
#install.packages("torchvision")
#install.packages("luz")
```

```{r}
# load packages
library(ggplot2)
library(gridExtra)
library(imager)
library(jpeg)
library(magick)
library(EBImage)
library(grid)
library(dplyr)
library(abind)
```

If using Mac OSX: imager needs [Xquartz](https://www.xquartz.org/)
- If using homebrew: `brew install --cask xquartz`

# Diagnosing Pneumonia from Chest X-Rays
## Parsing the data

Today, we are going to look at a series of chest X-rays from children (from [this paper](https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5)) and see if we can accurately diagnose pneumonia from them.  

Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for inclusion. In order to account for any grading errors, the evaluation set was also checked by a third expert.

We can download, unzip, then inspect the format of this dataset as follows:

"https://drive.google.com/file/d/1-5JWvxlPayoXOuIhNPWkI0vGh4I8JfxF/view?usp=sharing"

you can **unzip** the file in a data directory, using the following code:
*uncomment it*

```{r}
# Specify the path to the zip file
#zip_file <- "data/lab3_chest_xray.zip"

# Specify the destination directory to extract the files
#destination_dir <- "data"

# Extract the zip file
#unzip(zip_file, exdir = destination_dir)
```

As with every data analysis, the first thing we need to do is learn about our data. If we are diagnosing pneumonia, we should use the internet to better understand what pneumonia actually is and what types of pneumonia exist.

**0_** What is pneumonia and what is the point/benefit of being able to identify it from X-rays automatically?

Pneumonia is an infection that inflames the air sacs in one or both lungs. The air sacs may fill with fluid or pus, causing symptoms such as:

- Coughing
- Fever
- Chills
- Difficulty Breathing
- Chest Pain
- Fatigue
- Shortness of breath

Benefits of identifying Pneumonia from X-Rays automatically:

**1. Early Detection: **Automatic detection through X-rays allows for faster diagnosis which is crucial for starting treatment early. It can help in improving the patient outcomes and reduce the risk of complications.

**2. Accuracy and Consistency: **Utilizing ML algorithms can reduce the risk of human error in interpreting the X-rays, leading to more accurate diagnosis, providing consistent evaluations which is beneficial in busy or resource limited settings.

**3. Efficiency: **It allows healthcare providers to analyze the large volume of X-rays in short amount of time thereby helping the medical professionals to focus on complex cases or other critical tasks.

**4. Accessibility: **It helps with accessibility in the rural or under-resourced areas where there are limited skilled medical professionals. It can also enable remote diagnosis and consultation.

**5. Cost Effective and Clinical Support: **Potentially lowers the healthcare costs by improving diagnostic efficiency and reducing the need of repeated &  unnecessary imaging. Provides clinicians with decision support tools that can highlight the areas of concern of X-rays helping with their diagnostic process.

In summary, automatic identification of pneumonia from X-rays leverages technology to enhance the speed, accuracy, and accessibility of medical diagnostics, ultimately improving patient care and optimizing healthcare resources.

# Exploring dataset

In the provided code snippet, we are exploring the dataset directory structure for a chest x-ray image classification task. 

Make sure to put lab3_chest_xray directory in your R project directory.

```{r}
data_folder = "lab3_chest_xray"

files <- list.files(data_folder, full.names=TRUE, recursive=TRUE)
sort(sample(files, 20))
```

In the output above, you can see the filenames for all the chest X-rays. Look carefully at the filenames for the X-rays showing pneumonia (i.e., those in `{train,test}/PNEUMONIA`).

**1_** Do these filenames tell you anything about pneumonia? Why might this make predicting pneumonia more challenging?

The filenames for the chest X-rays showing pneumonia are structured in a way that includes certain keywords such as "bacteria" and "virus" along with a unique identifier. Some examples of the output are:

- "lab3_chest_xray/test/PNEUMONIA/person17_virus_48.jpeg"
- "lab3_chest_xray/train/PNEUMONIA/person1096_virus_1816.jpeg"
- "lab3_chest_xray/train/PNEUMONIA/person1196_bacteria_3146.jpeg"
- "lab3_chest_xray/train/PNEUMONIA/person1259_bacteria_3217.jpeg"

So the insights we can obtain from the filenames are:

**Type of Pathogen: **The filenames indicate whether the pneumonia is caused by a virus or bacteria. This can provide additional information about the nature of the infection, which might be relevant for medical professionals.

**Unique Identifiers: **Each file has a unique identifier that helps distinguish between different X-rays, which is important for managing and referencing large datasets.

Challenges in predicting Pneumonia from filenames:

**Generalization: **A model trained on a dataset with specific pathogen-related labels may struggle to generalize to new data where these labels are not present or are different. Ensuring the model can accurately predict pneumonia in diverse scenarios is essential for its real-world application.

**Potential Bias: **If the dataset is not balanced (e.g., more bacterial pneumonia cases than viral), the model might develop a bias towards the more prevalent class. Ensuring a balanced dataset is crucial for developing an unbiased predictive model.

**Pathogen-Specific features: **Pneumonia caused by bacteria may present differently on an X-ray compared to viral pneumonia. A predictive model needs to be able to distinguish these subtle differences to be effective.

**Lack of detailed information: **While the filenames indicate the type of pathogen, they do not provide detailed clinical information, such as the severity of the infection, patient demographics, or co-existing conditions. Predicting pneumonia based solely on the X-ray images might be challenging without this contextual information.

The dataset folder is assumed to contain three subfolders: train, test, and validate. Each of these subfolders further contains two folders representing the two classes or labels in our classification task: "normal" and "pneumonia".
The code provides a way to explore the dataset directory structure and obtain information about the number of images and their distribution across different classes. This information can be helpful in understanding the dataset and planning the subsequent steps of the image classification task.

```{r}
# Exploring dataset
base_dir <- "lab3_chest_xray"

train_pneumonia_dir <- file.path(base_dir, "train", "PNEUMONIA")
train_normal_dir <- file.path(base_dir, "train", "NORMAL")

test_pneumonia_dir <- file.path(base_dir, "test", "PNEUMONIA")
test_normal_dir <- file.path(base_dir, "test", "NORMAL")

val_normal_dir <- file.path(base_dir, "validate", "NORMAL")
val_pneumonia_dir <- file.path(base_dir, "validate", "PNEUMONIA")

train_pn <- list.files(train_pneumonia_dir, full.names = TRUE)
train_normal <- list.files(train_normal_dir, full.names = TRUE)

test_normal <- list.files(test_normal_dir, full.names = TRUE)
test_pn <- list.files(test_pneumonia_dir, full.names = TRUE)

val_pn <- list.files(val_pneumonia_dir, full.names = TRUE)
val_normal <- list.files(val_normal_dir, full.names = TRUE)

cat("Total Images:", length(c(train_pn, train_normal, test_normal, test_pn, val_pn, val_normal)), "\n")
cat("Total Pneumonia Images:", length(c(train_pn, test_pn, val_pn)), "\n")
cat("Total Normal Images:", length(c(train_normal, test_normal, val_normal)), "\n")

```
# Creating training datasets

The provided code segment focuses on creating datasets for training, testing, and validation, as well as assigning labels to the corresponding datasets. Additionally, the code shuffles the data to introduce randomness in the order of the samples. Here's a breakdown of the code:

`train_dataset`: Combines the lists `train_pn` and `train_normal` to create a single dataset for training.

`train_labels`: Creates a vector of labels for the training dataset by repeating "pneumonia" for the length of `train_pn` and "normal" for the length of `train_normal`.

`shuffled_train_dataset`, `shuffled_train_labels`: Extracts the shuffled training dataset and labels from the shuffled `train_data` data frame.

By creating datasets and assigning labels, this code prepares the data for subsequent steps, such as model training and evaluation. The shuffling of the data ensures that the samples are presented in a random order during training, which can help prevent any biases or patterns that may exist in the original dataset.
```{r}
train_dataset <- c(train_pn, train_normal)
train_labels <- c(rep("pneumonia", length(train_pn)), rep("normal", length(train_normal)))

test_dataset <- c(test_pn, test_normal)
test_labels <- c(rep("pneumonia", length(test_pn)), rep("normal", length(test_normal)))

val_dataset <- c(val_pn, val_normal)
val_labels <- c(rep("pneumonia", length(val_pn)), rep("normal", length(val_normal)))

# Create a data frame with the dataset and labels
train_data <- data.frame(dataset = train_dataset, label = train_labels)
test_data <- data.frame(dataset = test_dataset, label = test_labels)
val_data <- data.frame(dataset = val_dataset, label = val_labels)

# Shuffle the data frame
train_data <- train_data[sample(nrow(train_data)), ]
test_data <- test_data[sample(nrow(test_data)), ]
val_data <- val_data[sample(nrow(val_data)), ]

# Extract the shuffled dataset and labels
shuffled_train_dataset <- train_data$dataset
shuffled_train_labels <- train_data$label

shuffled_test_dataset <- test_data$dataset
shuffled_test_labels <- test_data$label

shuffled_val_dataset <- val_data$dataset
shuffled_val_labels <- val_data$label
```

```{r}
#showing a file name from test set
cat("file name: ", shuffled_train_dataset[5], "\nlabel: ", shuffled_train_labels[5])
```

# Data Visualization

Let’s inspect a couple of these files as it is always worth looking directly at data.

```{r}
# Create a list to store the ggplot objects
plots <- list()

# Iterate through the images and labels
for (i in 1:4) {
  
  image <- readImage(shuffled_train_dataset[i])
  
  # Create a ggplot object for the image with the corresponding label
  plot <- ggplot() +
    theme_void() +
    annotation_custom(
      rasterGrob(image, interpolate = TRUE),
      xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf
    )
  
  # Add the ggplot object to the list
  plots[[i]] <- plot
}

# Arrange the plots in a 2x2 grid
grid.arrange(grobs = plots, nrow = 2, ncol = 2)
```

**2_** From looking at only 3 images do you see any attribute of the images that we may have to normalise before training a model?

Looking at the above images we can inspect some attributes that may need normalization before training a model. Here are some key points to consider:

**Intensity Levels: **X-ray images can have varying intensity levels due to different exposure settings, patient sizes, and positions. Normalizing the intensity levels ensures consistency across all images.

**Image Size: **We can ensure that all images are resized to the same dimensions. This is crucial for feeding the images into a neural network, as it expects input of a fixed size.

**Contrast and Brightness: **We can standardize the brightness and contrast between the images for the better performance of our neural network. This can be achieved using histogram equalization techniques. 

**Orientation: **The orientation of the images can be consistent. For example, all images can be aligned such that the patient is facing the same direction. This can be achieved using image augmentation techniques.

**Noise Reduction: **X-ray images may contain noise and artifacts. Applying noise reduction techniques and removing artifacts can help improve the quality of the images.

So normalization steps to consider are resizing (images resized to same dimensions), scaling intensity values (scale the pixels to [0,-1]), histogram equalization (to improve brightness and contrast), data augmentation (rotation/flipping to create more robust model).

# Data Pre-processing

The provided code segment presents a function called `process_images` that performs several preprocessing steps on the images. Here's an explanation of each preprocessing step and its purpose:

1. Loading the "imager" library: This line imports the "imager" library, which provides functions for image processing.

2. Setting the desired image size: The variable `img_size` is initialized to 224, indicating the desired size (both width and height) of the processed images. This step ensures that all images are resized to a consistent size.

3. Initializing an empty list for processed images: The variable `X` is initialized as an empty list that will store the processed images.

4. Looping through each image path: The function iterates over each image path in the `shuffled_dataset` input, which represents the paths to the shuffled images.

5. Loading the image: The `imager::load.image()` function is used to read and load the image from its file path.

6. Normalizing the image: The loaded image is divided by 255 to normalize its pixel values. This step scales the pixel values between 0 and 1, which is a common practice in image processing and deep learning.

7. Resizing the image: The `resize()` function from the "imager" library is employed to resize the normalized image to the desired `img_size`. Resizing the images to a consistent size is important for ensuring compatibility with the subsequent steps of the deep learning pipeline.

8. Appending the processed image to the list: The processed image, stored in the variable `img_resized`, is added to the `X` list using the `c()` function and the `list()` function. The resulting `X` list will contain all the processed images.

9. Returning the processed images: Finally, the `X` list, which now holds the processed images, is returned as the output of the `process_images` function.

These preprocessing steps are commonly performed in image classification tasks to prepare the images for training a deep learning model. Normalizing the pixel values and resizing the images ensure that they are in a consistent format and range, which facilitates the learning process of the model. Additionally, resizing the images to a fixed size allows for efficient batch processing and ensures that all images have the same dimensions, enabling them to be fed into the model's input layer.

```{r}
process_images <- function(shuffled_dataset) {
  
  img_size <- 224  # Desired image size
  
  # Initialize an empty list to store processed images
  X <- list()
  
  # Loop through each image path in shuffled_train_dataset
  for (image_path in shuffled_dataset) {
    # Read the image
    img <- imager::load.image(image_path)
    
    # Normalize the image
    img_normalized <- img / 255
    
    # Resize the image
    img_resized <- resize(img_normalized, img_size, img_size)
    
    # Append the processed image to the list
    X <- c(X, list(img_resized))
  }
  
  return(X)
}
```

In this section, by using the process_images function we will do preprocessing on the training, testing, and validation images.

Then we will encode the labels:
The ifelse() function is utilized to encode the labels. If a label in shuffled_train_labels, shuffled_test_labels, or shuffled_val_labels is "normal," it is assigned a value of 1. Otherwise, if the label is "pneumonia," it is assigned a value of 2. This encoding scheme allows for easier handling of the labels in subsequent steps.

Finally, We Convert labels to integer type:
The labels are converted to the integer data type using the as.integer() function. This ensures that the labels are represented as integers, which is the expected format for the target tensor when using the nn_cross_entropy_loss function.

It is important to note that when using the nn_cross_entropy_loss function in R, the target tensor is expected to have a "long" data type, which is equivalent to the integer type in R. Hence, the labels need to be converted to integers.

Furthermore, when working with the Torch package in R, it is essential to ensure that labels start from 1 instead of 0. In binary classification problems, the labels should be 1 and 2, representing the two classes. This adjustment is necessary to avoid errors when using the labels as indices for the output tensor.


```{r}
train_X <- process_images(shuffled_train_dataset)
test_X <- process_images(shuffled_test_dataset)
val_X <- process_images(shuffled_val_dataset)

train_y <- ifelse(shuffled_train_labels == "normal", 1, 2)
test_y <- ifelse(shuffled_test_labels == "normal", 1, 2)
val_y <- ifelse(shuffled_val_labels == "normal", 1, 2)

train_y <- as.integer(train_y)
test_y <- as.integer(test_y)
val_y <- as.integer(val_y)
```

Now let's have an other look at images after doing pre processing.

```{r}
# Create a list to store the ggplot objects
plots <- list()

# Iterate through the images and labels
for (i in 1:4) {
  if (train_y[i] == 0) {
    label <- "Normal"
  } else {
    label <- "Pneumonia"
  }
  
  # Create a ggplot object for the image with the corresponding label
  plot <- ggplot() +
    theme_void() +
    ggtitle(label) +
    annotation_custom(
      rasterGrob(train_X[[i]], interpolate = TRUE),
      xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf
    )
  
  # Add the ggplot object to the list
  plots[[i]] <- plot
}

# Arrange the plots in a 2x2 grid
grid.arrange(grobs = plots, nrow = 2, ncol = 2)
```

Let's count and display the number of images in each dataset to examine the distribution of labels in the data. This will help us understand the ratio of "normal" and "pneumonia" labels in the dataset.

```{r}
# Combine train, test, and val vectors into a single data frame
df <- data.frame(
  Data = rep(c("Train", "Test", "Val"), times = c(length(train_y), length(test_y), length(val_y))),
  Value = c(train_y, test_y, val_y)
)

# Create a single bar plot with facets
fig <- ggplot(df, aes(x = Value)) +
  geom_bar() +
  ylim(0, 510) +
  facet_wrap(~Data, ncol = 3)

# Arrange the plot
grid.arrange(fig, nrow = 1)

```
As you can see, the provided dataset is completely balanced in all sets.
**3_** if the dataset was not balanced, what kind of techniques could be useful?

If the dataset was not balanced, meaning that there is a significant difference in the number of images for the "normal" and "pneumonia" classes, several techniques could be employed to address this imbalance:

**1.Resampling Methods: **

- Oversampling: Increase the number of instances in the minority class by replicating existing instances or creating synthetic ones. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be used.

- Undersampling: Reduce the number of instances in the majority class by randomly removing instances. This can help balance the dataset but might lead to loss of important information.

**2.Data Augmentation: **Apply transformations (e.g., rotations, translations, flipping) to the images in the minority class to create more varied instances. This increases the diversity of the training data and helps the model generalize better.

**3.Class Weights: **Assign higher weights to the minority class during training to make the model pay more attention to it. Most machine learning libraries allow setting class weights in the loss function.

**4.Ensemble Methods: **Use ensemble techniques like bagging and boosting that are more robust to class imbalances. Techniques like Balanced Random Forest and EasyEnsemble can help improve performance on imbalanced datasets.

**5.Hybrid Methods: **Combine oversampling of the minority class and undersampling of the majority class to balance the dataset.

**6.Anomaly Detection: **Treat the minority class as anomalies and use anomaly detection techniques to identify them.

**7.Generating Synthetic Data: **Use generative models like GANs (Generative Adversarial Networks) to create synthetic images for the minority class.


# Training

In the next step, we need to reshape the dataset to ensure it is in the appropriate format for feeding into deep learning models. Reshaping involves modifying the structure and dimensions of the data to match the expected input shape of the model. 

```{r}
train_X <- array(data = unlist(train_X), dim = c(1000, 224, 224, 1))
test_X <- array(data = unlist(test_X), dim = c(200, 224, 224, 1))
val_X <- array(data = unlist(val_X), dim = c(16, 224, 224, 1))
```

```{r}
print(dim(train_X))
print(length(train_y))
print(dim(test_X))
print(length(test_y))
print(dim(val_X))
print(length(val_y))
```
The last dimension of an image represents the color channels, typically RGB (Red, Green, Blue) in color images or grayscale in black and white images. In our case, since we are working with black and white images, there is only one channel, hence the value 1.

However, the deep learning framework expects the input tensor to have a specific shape, with the color channels as the second dimension. The desired shape is (batch_size x channels x height x width), but our current data has the shape (batch_size x height x width x channels).

To rearrange the dimensions of our data to match the expected shape, we use the aperm function in R. The aperm function allows us to permute the dimensions of an array. In this case, we are permuting the dimensions of train_X to change the order of the dimensions, so that the channels dimension becomes the second dimension.

```{r}
train_X <- aperm(train_X, c(1,4,2,3))
test_X <- aperm(test_X, c(1,4,2,3))
val_X <- aperm(val_X,c(1,4,2,3))

dim(train_X)
```

In order to train a Convolutional Neural Network (CNN), we will utilize the torch package in R. Torch is a powerful deep learning library that provides a wide range of functions and tools for building and training neural networks.

CNNs are particularly effective for image-related tasks due to their ability to capture local patterns and spatial relationships within the data. Torch provides a high-level interface to define and train CNN models in R.

By using torch, we can leverage its extensive collection of pre-built layers, loss functions, and optimization algorithms to construct our CNN architecture. We can define the network structure, specifying the number and size of convolutional layers, pooling layers, and fully connected layers.

```{r}
# Load the torch package
library(torch)
library(torchvision)
library(luz)
```

In this code, we are defining a custom dataset class called "ImageDataset" to encapsulate our training, testing, and validation data. The purpose of the dataset class is to provide a structured representation of our data that can be easily consumed by deep learning models.

The "ImageDataset" class has three main functions:
1. "initialize": This function is called when creating an instance of the dataset class. It takes the input data (X) and labels (y) as arguments and stores them as tensors.
2. ".getitem": This function is responsible for retrieving a single sample and its corresponding label from the dataset. Given an index (i), it returns the i-th sample and label as tensors.
3. ".length": This function returns the total number of samples in the dataset.

After defining the dataset class, we create instances of it for our training, testing, and validation data: "train_dataset", "test_dataset", and "val_dataset". We pass the respective input data and labels to each dataset instance.

Next, we create dataloader objects for each dataset. A dataloader is an abstraction that allows us to efficiently load and iterate over the data in batches during the training process. The batch size (16 in this case) determines the number of samples that will be processed together in each iteration. It helps in optimizing memory usage and can speed up the training process by leveraging parallel computation.

Finally, the code visualizes the size of the first batch by calling "batch[[1]]$size()". This can be useful for understanding the dimensions of the data and ensuring that the input shapes are consistent with the network architecture.

```{r}
# Define a custom dataset class
ImageDataset <- dataset(
  name = "ImageDataset",
  initialize = function(X, y) {
    # Store the data as tensors
    self$data <- torch_tensor(X)
    self$labels <- torch_tensor(y)
  },
  .getitem = function(i) {
    # Return a single sample and label
    x <- self$data[i,,,]
    y <- self$labels[i]
    list(x = x, y = y)
  },
  .length = function() {
    # Return the number of samples
    dim(self$data)[1]
  }
)

# Create a dataset object from your data
train_dataset <- ImageDataset(train_X, train_y)
test_dataset <- ImageDataset(test_X, test_y)
val_dataset <- ImageDataset(val_X, val_y)

# Create a dataloader object from your dataset
train_dataloader <- dataloader(train_dataset, batch_size = 16)
test_dataloader <- dataloader(test_dataset, batch_size = 16)
val_dataloader <- dataloader(val_dataset, batch_size = 16)

# Iterate over batches of data
batch = train_dataloader$.iter()$.next()

# Visualize the first batch size
batch[[1]]$size()
```
## creat the CNN model

The input image has one channel and a size of 224 x 224 pixels. The first convolutional layer has 32 filters with a kernel size of 3 x 3 and a stride of 1. The output of this layer has a size of 32 x 222 x 222. The second convolutional layer has 64 filters with the same kernel size and stride. The output of this layer has a size of 64 x 220 x 220. The max pooling layer has a kernel size of 2 x 2 and reduces the spatial dimensions by half. The output of this layer has a size of 64 x 110 x 110. The dropout layer randomly sets some elements to zero with a probability of 0.25. The flatten layer reshapes the output into a vector with a length of 774400. The first fully connected layer has 128 neurons and applies a ReLU activation function. The second dropout layer randomly sets some elements to zero with a probability of 0.5. The second fully connected layer has 2 neurons and produces the final output for the classification task.

Input Image: 1 channel, 224 x 224

| Layer Type    | Output Size    | Parameters      |
| ------------- | -------------- | --------------- |
| Conv2D        | 32 x 222 x 222 | 32 x 3 x 3      |
|               |                | (weights)       |
| Conv2D        | 64 x 220 x 220 | 64 x 3 x 3      |
|               |                | (weights)       |
| MaxPooling2D  | 64 x 110 x 110 | 2 x 2           |
|               |                | (kernel size)   |
| Dropout       | 64 x 110 x 110 | 0.25            |
|               |                | (dropout rate)  |
| Flatten       | 774400         | -               |
| FullyConnected| 128            | -               |
| (ReLU)        |                |                 |
| Dropout       | 128            | 0.5             |
|               |                | (dropout rate)  |
| FullyConnected| 2              | -               |



```{r}
net <- nn_module(
  "Net",
  
  initialize = function() {
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 1)
    self$dropout1 <- nn_dropout2d(0.25)
    self$dropout2 <- nn_dropout2d(0.5)
    self$fc1 <- nn_linear(774400, 128)  # Adjust the input size based on your image dimensions
    self$fc2 <- nn_linear(128, 2)             # Change the output size to match your classification task
  },
  
  forward = function(x) {
    x %>%                                        # N * 1 * 224 * 224
      self$conv1() %>%                           # N * 32 * 222 * 222
      nnf_relu() %>% 
      self$conv2() %>%                           # N * 64 * 220 * 220
      nnf_relu() %>% 
      nnf_max_pool2d(2) %>%                      # N * 64 * 110 * 110
      self$dropout1() %>% 
      torch_flatten(start_dim = 2) %>%           # N * 64 * 110 * 110 --> N * 774400
      self$fc1() %>%                             # N * 128
      nnf_relu() %>% 
      self$dropout2() %>% 
      self$fc2()                                 # N * 2 (change the output size to match your classification task)
  }
)

```

## Train model

```{r warning=FALSE}
# Set the number of epochs
num_epochs <- 1

train_loss <- numeric(num_epochs)
train_acc <- numeric(num_epochs)
test_loss <- numeric(num_epochs)
test_acc <- numeric(num_epochs)

# Loop through the epochs
for (epoch in 1:num_epochs) {
  # Perform training and validation for each epoch
  fitted <- net %>%
    setup(
      loss = nn_cross_entropy_loss(),
      optimizer = optim_adam,
      metrics = list(
        luz_metric_accuracy()
      )
    ) %>%
    fit(train_dataloader, epochs = 1, valid_data = test_dataloader)
  
  # Print the metrics for the current epoch
  cat("Epoch ", epoch, "/", num_epochs, "\n")
  cat("Train metrics: Loss: ", fitted$records$metrics$train[[1]]$loss, " - Acc: ", fitted$records$metrics$train[[1]]$acc, "\n")
  cat("Valid metrics: Loss: ", fitted$records$metrics$valid[[1]]$loss, " - Acc: ", fitted$records$metrics$valid[[1]]$acc, "\n")
  cat("\n")
  
  # Store the loss and accuracy values
  train_loss[epoch] <- fitted$records$metrics$train[[1]]$loss
  train_acc[epoch] <- fitted$records$metrics$train[[1]]$acc
  test_loss[epoch] <- fitted$records$metrics$train[[1]]$loss
  test_acc[epoch] <- fitted$records$metrics$valid[[1]]$acc
}

```
## Plot learning curves

```{r}
# Plot the train and test loss
loss_df <- data.frame(
  Epoch = 1:num_epochs,
  Train_Loss = train_loss,
  Test_Loss = test_loss
)
loss_plot <- ggplot(data = loss_df) +
  geom_line(aes(x = Epoch, y = Train_Loss, color = "Train Loss")) +
  geom_line(aes(x = Epoch, y = Test_Loss, color = "Test Loss")) +
  labs(x = "Epoch", y = "Loss") +
  scale_color_manual(values = c("Train Loss" = "blue", "Test Loss" = "red")) +
  theme_minimal()

# Plot the train and test accuracy
acc_df <- data.frame(
  Epoch = 1:num_epochs,
  Train_Accuracy = train_acc,
  Test_Accuracy = test_acc
)
acc_plot <- ggplot(data = acc_df) +
  geom_line(aes(x = Epoch, y = Train_Accuracy, color = "Train Accuracy")) +
  geom_line(aes(x = Epoch, y = Test_Accuracy, color = "Test Accuracy")) +
  labs(x = "Epoch", y = "Accuracy") +
  scale_color_manual(values = c("Train Accuracy" = "blue", "Test Accuracy" = "red")) +
  theme_minimal()

# Print the plots
print(loss_plot)
print(acc_plot)
```

**4_** Based on the training and test accuracy, is this model actually managing to classify X-rays into pneumonia vs normal? What do you think contributes to this? why?



It seems that the model is not training effectively the model is struggling to learn from the data and is not generalizing well to unseen examples. It suggests that the model may be too complex or the training process needs adjustments.

**5_** what is your suggestions to solve this problem? How could we improve this model?

# Data Augmentation
Data augmentation is one of the solutions to consider when facing training difficulties. Data augmentation involves applying various transformations or modifications to the existing training data, effectively expanding the dataset and introducing additional variations. This technique can help improve model performance and generalization by providing the model with more diverse examples to learn from.

By applying data augmentation, we can create new training samples with slight modifications, such as random rotations, translations, flips, zooms, or changes in brightness and contrast. These modifications can mimic real-world variations and increase the model's ability to handle different scenarios. Data augmentation increased dataset size, improved generalization, and reduced overfitting.

Below is an example that demonstrates how we can augment the data.

```{r}
img <- readImage(shuffled_train_dataset[5])

T_img <- torch_squeeze(torch_tensor(img)) %>% 
        # Randomly change the brightness, contrast and saturation of an image
        transform_color_jitter() %>%
        # Horizontally flip an image randomly with a given probability
        transform_random_horizontal_flip() %>%
        # Vertically flip an image randomly with a given probability
        transform_random_vertical_flip(p = 0.5)

image(as.array(T_img))
```

We can also add the data transformations to our dataloader:

```{r}
# Define a custom dataset class with transformations
ImageDataset_augment <- dataset(
  name = "ImageDataset",
  initialize = function(X, y, transform = NULL) {
    self$transform <- transform
    self$data <- X
    self$labels <- y
  },
  .getitem = function(i) {
    # Return a single sample and label
    x <- self$data[i,,,]
    x <- self$transform(x)
    y <- self$labels[i]
    
    list(x = x, y = y)
  },
  .length = function() {
    dim(self$data)[1]
  }
)

# Define the transformations for training data
train_transforms <- function(img) {
  img <- torch_squeeze(torch_tensor(img)) %>%
    transform_color_jitter() %>%
    transform_random_horizontal_flip() %>%
    transform_random_vertical_flip(p = 0.5) %>%
    torch_unsqueeze(dim = 1)
  
  return(img)
}

# Apply the transformations to your training dataset
train_dataset <- ImageDataset_augment(train_X, train_y, transform = train_transforms)
test_dataset <- ImageDataset(test_X, test_y)
val_dataset <- ImageDataset(val_X, val_y)

# Create a dataloader for training
train_dataloader <- dataloader(train_dataset, batch_size = 16)
test_dataloader <- dataloader(test_dataset, batch_size = 16)
val_dataloader <- dataloader(val_dataset, batch_size = 16)

# Iterate over batches of data
batch = train_dataloader$.iter()$.next()

# Visualize the first batch size
batch[[1]]$size()
```

**6_** What are the potential drawbacks or disadvantages of data augmentation?

Data augmentation is a powerful technique used to increase the diversity of training data without actually collecting new data. However, there are several potential drawbacks and disadvantages to consider:

**1.Overfitting on Augmented Data: **If the augmentation techniques are too simplistic or not varied enough, the model might learn to overfit on the specific transformations applied rather than generalizing to new, unseen data.

**2.Quality of Augmented Data: **Poorly designed augmentation strategies can introduce unrealistic or misleading variations that do not represent real-world data, potentially confusing the model and degrading its performance.

**3.Increased Training Time: **Data augmentation increases the size of the dataset, which can lead to longer training times and higher computational costs.

**4.Complexity in Implementation: **Designing effective augmentation strategies requires a good understanding of the data and the domain. It can be complex to implement and may require extensive experimentation to get right.

**5.Resource Intensive: **Applying augmentation techniques on-the-fly during training can be resource-intensive, requiring more memory and processing power.

**6.Potential for Bias Introduction: **If the augmentation techniques are not carefully designed, they can introduce biases. For example, certain rotations or flips might be more common in one class of data, leading to unintended biases in the model.

**7.Applicability: **Not all types of data are suitable for augmentation. While it works well for images, it might be less effective or more complex for other types of data such as text or time series.

**8.Data Integrity: **There is a risk that augmented data might not preserve the integrity of the original data. For example, certain augmentations might change the context of an image, making it less representative of the original class.


# Mobile net

Transfer learning is another technique in machine learning where knowledge gained from solving one problem is applied to a different but related problem. It involves leveraging pre-trained models that have been trained on large-scale datasets and have learned general features. By utilizing transfer learning, models can benefit from the knowledge and representations learned from these pre-trained models.

Torchvison provides versions of all the integrated architectures that have already been trained on the ImageNet dataset.

**7_** What is ImageNet aka “ImageNet Large Scale Visual Recognition Challenge 2012”? How many images and classes does it involve? Why might this help us?

ImageNet is a large-scale image dataset organized according to the WordNet hierarchy, in which each node of the hierarchy is depicted by hundreds and thousands of images. The goal of ImageNet is to provide researchers with a vast amount of images that are annotated for various objects, facilitating advancements in computer vision and machine learning.

ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 is one of the most well-known annual computer vision competitions. It involves developing algorithms for object detection and image classification at large scale. The challenge evaluates the performance of algorithms in terms of their ability to classify images into one of many object categories.

**Statistics of ILSVRC012**

- **Number of Images: **The dataset contains over 1.2 million training images, 50,000 validation images, and 150,000 testing images.

- **Number of Classes: **The dataset is annotated with 1,000 object categories.

**Importance and Impact of ImageNet**

- ImageNet provides a standard benchmark for evaluating and comparing the performance of various image classification and object detection algorithms. It has played a pivotal role in the development and validation of many state-of-the-art deep learning models, including convolutional neural networks (CNNs).

- The challenge led to significant advancements in convolutional neural networks. The success of models like AlexNet (which won ILSVRC 2012) demonstrated the effectiveness of deep learning techniques and spurred further research and development. Many deep learning architectures, such as VGG, GoogLeNet, ResNet, and others, were validated and improved using the ImageNet dataset.

- Models trained on ImageNet are often used as pre-trained models for transfer learning in other tasks. The large number of images and categories helps these models learn rich and diverse feature representations that can be transferred to other tasks with smaller datasets. Pre-trained ImageNet models can be used for feature extraction, providing robust features that improve the performance of downstream tasks.

- The large number of categories and the diversity of images in ImageNet help models trained on this dataset to generalize well to a wide variety of visual recognition tasks. Models that perform well on ImageNet typically exhibit robustness to different types of visual data, making them useful for various applications beyond the original challenge.

**Why might this help us? **

- Using pre-trained models on ImageNet as a starting point for training can significantly reduce training time and improve performance, especially when working with limited data. Leveraging the rich feature representations learned from ImageNet can lead to improved accuracy in detecting and classifying images, including medical images like chest X-rays for pneumonia detection.

- Using pre-trained ImageNet models for feature extraction can provide high-quality features for other tasks, helping to build more accurate models.

- Pre-trained models can be fine-tuned to adapt to specific tasks such as medical image classification, where collecting large annotated datasets is challenging.

- ImageNet-trained models serve as a baseline for performance benchmarking, allowing researchers to compare their methods against well-established standards.

MobileNet is a pre-trained model that can be effectively utilized in transfer learning scenarios. Do some research about this model.

**8_** Why do you think using this architecture in this practical assignment can help to improve the results? 
Hint: See MobileNet publication

MobileNet is a family of neural network architectures optimized for mobile and embedded vision applications. It is particularly known for its efficiency and low computational cost, making it an excellent choice for resource-constrained environments.

**Key Features of MobileNet**

- **Depthwise Separable Convolutions: **MobileNet uses depthwise separable convolutions, which split the standard convolution into two layers: depthwise convolution and pointwise convolution. This reduces the number of parameters and computational cost significantly without sacrificing too much accuracy.

- **Bottleneck Design: **MobileNetV2 introduces an inverted residual structure with linear bottlenecks. This design helps in maintaining high performance while reducing computational complexity.

- **Squeeze-and-Excitation Blocks: **MobileNetV3 incorporates squeeze-and-excitation blocks, which enhance the network's ability to model channel-wise dependencies, improving feature representation capabilities.

**Advantages of MobileNet for this practical**

- **Efficiency and Speed: **MobileNet is designed to be computationally efficient, making it suitable for applications with limited resources such as mobile or embedded systems. This efficiency allows for faster training and inference times.

- **Transfer Learning: **MobileNet can be pre-trained on large datasets like ImageNet and then fine-tuned on specific tasks such as pneumonia detection from chest X-rays. This transfer learning approach leverages the knowledge gained from a vast amount of data, improving model performance on smaller, domain-specific datasets.

- **Scalability: **MobileNet models can be scaled in terms of width (number of channels) and resolution (input image size) to balance between latency and accuracy based on the application's requirements.

- **High Performance: **Despite its lightweight architecture, MobileNet achieves competitive performance. It offers a good balance between model size, computational cost, and accuracy, making it a practical choice for real-world applications. 

Practical benefits are by using pre-trained MobileNet models and fine-tuning them for pneumonia detection, we can achieve high accuracy even with limited training data. This is crucial for medical applications where data collection is often challenging. The efficiency of MobileNet ensures that the model can be deployed quickly on various devices, including those with limited computational power. This is beneficial for real-time applications such as point-of-care diagnostics. The pre-trained weights from large datasets help in generalizing better to new data, reducing the risk of overfitting. This is particularly important in medical imaging, where the diversity in data is essential for robust model performance.


**9_** How many parameters does this network have? How does this compare to better performing networks available in torch?

MobileNetV2 has approximately 3.4 million parameters when configured with a width multiplier of 1.0 and an input size of 224x224 pixels. This makes it significantly more efficient in terms of both memory and computational requirements compared to many other architectures.

**Comparison with Other Networks in PyTorch**

When comparing MobileNetV2 with other high-performing networks available in PyTorch, such as ResNet, VGG, and DenseNet, the following differences emerge:

**1.ResNet (eg: RestNet50): **ResNet-50 has around 25 million parameters, which is significantly higher than MobileNetV2. ResNet models generally achieve higher accuracy on large-scale image classification tasks but at the cost of increased computational resources and longer training times.

**2.VGG (eg: VGG-16): **VGG-16 has approximately 138 million parameters, making it one of the more parameter-heavy architectures. VGG models are known for their simplicity and effectiveness but are not as efficient in terms of computational cost and memory usage.

**3.DenseNet (eg: DenseNet121): **DenseNet-121 has around 8 million parameters, which is still more than double the parameters of MobileNetV2. DenseNet models provide good accuracy and efficient gradient flow due to dense connections, but they also require more computational resources compared to MobileNetV2.

**Benefits of using MobileNetV2 for Pneumonia Detection**

MobileNetV2’s low parameter count makes it highly efficient, enabling faster training and inference times. This is particularly beneficial in resource-constrained environments like mobile devices or edge computing applications. It can leverage pre-trained weights from large datasets like ImageNet, allowing for effective transfer learning. This can significantly enhance performance on specific tasks such as pneumonia detection from chest X-rays by utilizing the rich feature representations learned from a diverse dataset. The architecture of MobileNetV2 can be adjusted using the width multiplier to trade-off between accuracy and computational cost, making it adaptable to various hardware constraints and application requirements. MobileNetV2 has been proven effective in various real-world applications, including object detection and image classification on mobile and embedded devices, which underscores its practicality for tasks like medical image analysis.

In conclusion, using MobileNetV2 for our practical assignment can help improve the results by offering a balanced trade-off between computational efficiency and model accuracy, making it well-suited for both development and deployment in real-world medical imaging applications.

In the following you can see an example of how we can load pre-trained models in our codes.

```{r}
# Load the pre-trained MobileNet model

mobilenet<-model_mobilenet_v2(pretrained = TRUE)

# Modify the last fully connected layer to match your classification task

#in_features <- mobilenet$classifier$in_features
mobilenet$classifier <- nn_linear(224*224*3, 2)   # Adjust the output size based on your classification task
mobilenet
```
**10_** Using the provided materials in this practical, train a different network architecture. Does this perform better?

# Useful links
https://medium.com/@kemalgunay/getting-started-with-image-preprocessing-in-r-52c7d153b381
https://cran.r-project.org/web/packages/magick/vignettes/intro.html
https://www.datanovia.com/en/blog/easy-image-processing-in-r-using-the-magick-package/
https://dahtah.github.io/imager/imager.html
https://rdrr.io/github/mlverse/torchvision/api/
https://github.com/brandonyph/Torch-for-R-CNN-Example
